{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a114c7d2-a1b8-4e6c-b739-439384ac7927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeout\n",
    "import time\n",
    "\n",
    "# Years To Be Scraped\n",
    "SEASONS = list(range(2016, 2026))\n",
    "\n",
    "SEASONS\n",
    "\n",
    "# Directoryâ€™s To Store Scraped Standings And Box Scores\n",
    "DATA_DIR = \"data\"\n",
    "STANDINGS_DIR = os.path.join(DATA_DIR, \"standings\") # data -> standings\n",
    "SCORES_DIR = os.path.join(DATA_DIR, \"scores\") # data -> scores\n",
    "\n",
    "# Asynchronous Funtion To Use Playwright Library\n",
    "# Function To Get The Specific HTML Specified In The Selector From A Given URL \n",
    "async def get_html(url, selector, sleep=5, retries=3):\n",
    "    html = None\n",
    "    for i in range(1, retries+1): # Retries Scraping Process A Maximum Of 3 Times\n",
    "        time.sleep(sleep * i) # Sleeps/Pauses After Attempted Scraping To Prevent/Work Around Server Ban\n",
    "\n",
    "        # Tries To Run Code Unless There Is An Error Described Bellow \n",
    "        try:\n",
    "            async with async_playwright() as p:\n",
    "                # If Issues Occur Could Replace .chromium. With .firefox. To Use Fire Fox Instead\n",
    "                browser = await p.chromium.launch() # Opens A Chromium (Open Source Verson Of Chrome) Browser\n",
    "                page = await browser.new_page() # Opens A New Tab In Browser\n",
    "                await page.goto(url) # Sends Tab To The Given url\n",
    "                print(await page.title()) #Prints The Pages Title\n",
    "                html = await page.inner_html(selector) # Grabs A Specific Piece Of The Pages HTML\n",
    "                \n",
    "        # If Playwright Timeout Occurs Prints URL On Which The Error Occured And Increments The Number Of Retries Returning To The Top Of The For Loop\n",
    "        except PlaywrightTimeout:\n",
    "            print(f\"Timeout error on {url}\")\n",
    "            continue\n",
    "\n",
    "        # Breaks Loop If The Scrape Is Successful\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return html # Returns The Pages HTML\n",
    "\n",
    "# Function To Scrape The href And Anchor Tags From A Single Season Using The HTML Returned By The get_html Function\n",
    "async def scrape_season(season):\n",
    "    url = f\"https://www.basketball-reference.com/leagues/NBA_{season}_games.html\" # URL To Be Scraped, Itterating Through Seasons \n",
    "    \n",
    "    # Calls get_html Function\n",
    "    html = await get_html(url, \"#content .filter\") #Selector First Looks For The Element With The ID Content (Hashtag Means Find An Element By ID) Then Inside That Element Find The Element With The Class Filter And Returns The Given HTML \n",
    "    #This Function Will Return All The Anchor Tags And href Links With The Months Of The Seasons\n",
    "    \n",
    "    soup = BeautifulSoup(html) # Processes With Beautiful Soup Library\n",
    "    links = soup.find_all(\"a\") # Finds All The Anchor Tags Returned From The get_html Function Above\n",
    "    href = [l[\"href\"] for l in links] # Finds All The href Links Returned From The get_html Function Above\n",
    "    standings_pages = [f\"https://basketball-reference.com{l}\" for l in href] # Completes All The href Links To Be Active\n",
    "\n",
    "    # Itterates Through Each One Of The Standings Pages\n",
    "    for url in standings_pages:\n",
    "        # Saves The Scaped Data In The Standings Directory With A Specific File Name\n",
    "        save_path = os.path.join(STANDINGS_DIR, url.split(\"/\")[-1])\n",
    "\n",
    "        # If The Scaped Data Already Exists Do Not Scape It Again\n",
    "        if os.path.exists(save_path):\n",
    "            continue\n",
    "\n",
    "        # Runs The get_html Function To Get The Schedule Table Within A Given Month\n",
    "        html = await get_html(url, \"#all_schedule\")  \n",
    "\n",
    "        # Opens The File In Write Mode   \n",
    "        with open(save_path, \"w+\") as f:\n",
    "            f.write(html) # Writes HTML To The File\n",
    "\n",
    "# Loops Through Each Season\n",
    "for season in SEASONS:\n",
    "    # Opens Function To Scrape The Given Season\n",
    "    await scrape_season(season)\n",
    "\n",
    "# List All The Files In The Standings Directory \n",
    "standings_files = os.listdir(STANDINGS_DIR)\n",
    "\n",
    "# Function To Scrape The Contenet Tag In The Boxscore Of A Game \n",
    "async def scrape_game(standings_file):\n",
    "\n",
    "    # Opens An Individual Standings File Starting With The First In The List\n",
    "    with open(standings_file, 'r') as f:\n",
    "        html = f.read()\n",
    "    \n",
    "    soup = BeautifulSoup(html) # Processes With Beautiful Soup Library\n",
    "    links = soup.find_all(\"a\") # Gets All The A Tags With Individual Game Box Scores\n",
    "    hrefs = [l.get(\"href\") for l in links] # Filters HTML To Get All HREF Links \n",
    "    box_scores = [l for l in hrefs if l and \"boxscore\" in l and \".html\" in l] # Filters All HREF Links To Get All Boxscore Links\n",
    "    box_scores = [f\"https://www.basketball-reference.com{l}\" for l in box_scores] # Creates Full Link To Boxscores\n",
    "    \n",
    "    # Loops Through Each Box Score\n",
    "    for url in box_scores:\n",
    "        # Saves The Scaped Data In The Scores Directory With A Specific File Name\n",
    "        save_path = os.path.join(SCORES_DIR, url.split(\"/\")[-1])  \n",
    "    \n",
    "        # If The Scaped Data Already Exists Do Not Scape It Again\n",
    "        if os.path.exists(save_path):\n",
    "            continue\n",
    "    \n",
    "        # Runs The get_html Function To Get The Content Tag In The Box Score HTML\n",
    "        html = await get_html(url, \"#content\")\n",
    "    \n",
    "        # Of Download Fails Continue The Loop\n",
    "        if not html:\n",
    "            continue\n",
    "    \n",
    "        # Opens The File In Write Mode   \n",
    "        with open(save_path, \"w+\") as f:\n",
    "            f.write(html) # Writes HTML To The File    \n",
    "\n",
    "#Filters Out Any Files That Are Not The Monthly Standings \n",
    "standings_files = [s for s in standings_files if \".html\" in s]\n",
    "\n",
    "# Loops Through Each Standings File\n",
    "for f in standings_files:\n",
    "    filepath = os.path.join(STANDINGS_DIR, f)\n",
    "\n",
    "    # Opens Function To Scrape The Given Games\n",
    "    await scrape_game(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2415034c-dcb5-4c28-b922-f977c5a0c5da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
